# Entity Classification for Banking Metadata Extraction

## ðŸŽ¯ Project Goal
Research and evaluate NLP approaches for extracting and classifying entities in banking documents to generate rich metadata for a central indexing hub.

**Success Criteria:**
- â‰¥85% entity extraction accuracy
- â‰¥80% classification accuracy
- Handle unknown entities with â‰¥70% confidence
- Process <5 seconds per document

## ðŸ“‚ Project Structure

```
Risk-analysis/
â”‚
â”œâ”€â”€ ðŸ”¬ EVALUATION SCRIPTS (One per NLP approach)
â”‚   â”œâ”€â”€ evaluate_spacy.py          # spaCy baseline - Fast (4ms)
â”‚   â”œâ”€â”€ evaluate_zero_shot.py      # Zero-Shot Classification - Handles unknowns (500-1000ms)
â”‚   â”œâ”€â”€ evaluate_finbert.py        # FinBERT - Finance-specific (50-100ms)
â”‚   â””â”€â”€ evaluate_compare.py        # Run all approaches and compare results
â”‚
â”œâ”€â”€ ðŸ› ï¸ SHARED UTILITIES
â”‚   â””â”€â”€ utils_nlp.py               # Common functions for all evaluation scripts
â”‚
â”œâ”€â”€ ðŸ“Š DATASET
â”‚   â””â”€â”€ banking_ner_dataset.csv    # 81 labeled examples (7 entity types)
â”‚
â”œâ”€â”€ ðŸ“š DOCUMENTATION
â”‚   â”œâ”€â”€ README.md                  # This file - Project overview
â”‚   â”œâ”€â”€ QUICK_REFERENCE.md         # Command cheatsheet
â”‚   â””â”€â”€ NLP_LIBRARY_COMPARISON.md  # Detailed library comparison
â”‚
â”œâ”€â”€ ðŸ“¦ ARCHIVE
â”‚   â”œâ”€â”€ archive/                   # Different use case datasets
â”‚   â””â”€â”€ _archive_legacy/           # Legacy code (reference only)
â”‚
â””â”€â”€ ðŸ“ OUTPUT (generated after running)
    â”œâ”€â”€ results_spacy.json
    â”œâ”€â”€ results_zero_shot.json
    â”œâ”€â”€ results_finbert.json
    â””â”€â”€ comparison_results_*.csv
```

## ðŸ“Š Phase 1 Results (spaCy Baseline)

| Metric | Result | Status |
|--------|--------|--------|
| Entity Extraction Accuracy | 93.8% | âœ… Excellent |
| Entity Classification Accuracy | 67.9% | âš ï¸ Needs improvement |
| Well-known entities | 66.7% | âš ï¸ Needs improvement |
| Unknown/local entities | 73.3% | âš ï¸ Needs improvement |
| Processing Speed | 4.2 ms/doc | âœ… Production-ready |
| Throughput | 237 docs/sec | âœ… Fast |

**Key Findings:**
- âœ… **Strengths:** Very fast, good at extraction, 100% accuracy on banks/hospitals/universities
- âŒ **Weaknesses:** Poor on unknown entities, low accuracy on e-commerce (0%), money (28.6%), dates (50%)

## ðŸš€ Next Steps

### Phase 2: Advanced NLP Approaches â­ï¸
- [ ] Run Zero-Shot Classification evaluation
- [ ] Run FinBERT evaluation  
- [ ] Compare all approaches side-by-side
- [ ] Analyze accuracy vs speed trade-offs

### Phase 3: Hybrid System (Production) â­ï¸
- [ ] Build hybrid metadata extraction pipeline:
  - spaCy for fast extraction (93.8% found)
  - Zero-Shot for unknown entity classification
  - Knowledge base for top 100 frequent entities
  - **Expected:** 88-92% overall accuracy, 50-100ms speed

## ðŸ’¡ Key Benefits

âœ… **Clean Separation** - Each NLP approach in its own file  
âœ… **No Duplication** - Shared utilities in `utils_nlp.py`  
âœ… **Easy Testing** - Test any approach independently  
âœ… **Easy Comparison** - `evaluate_compare.py` orchestrates all  
âœ… **Extensible** - Easy to add new approaches in future

## ðŸ†˜ Troubleshooting

**Problem:** `Missing 'transformers'`  
**Solution:** `pip install transformers torch`

**Problem:** `spaCy model not found`  
**Solution:** `python -m spacy download en_core_web_md`

**Problem:** Script takes too long  
**Solution:** Zero-Shot is slow (5-10 min for full dataset). Use `evaluate_spacy.py` for quick testing.

## ðŸ“š Additional Documentation

- **QUICK_REFERENCE.md** - Command cheatsheet and quick guide
- **NLP_LIBRARY_COMPARISON.md** - Detailed comparison of NLTK, spaCy, Hugging Face Transformers, Gensim, Flair, Stanza, and Cloud APIs
- **_archive_legacy/** - Legacy code kept for reference only

## ðŸš€ Quick Start

### 1. Install Dependencies
```bash
# Install required packages
pip install transformers torch spacy pandas

# Download spaCy model
python -m spacy download en_core_web_md
```

### 2. Run Individual NLP Approach
```bash
cd "/Applications/Temasek Polytechnic/KM-MP/Risk-analysis"

# Option 1: spaCy (fastest, 4ms per document)
python evaluate_spacy.py

# Option 2: Zero-Shot (best for unknown entities, 500-1000ms)
python evaluate_zero_shot.py

# Option 3: FinBERT (finance-specific, 50-100ms)
python evaluate_finbert.py
```

### 3. Compare All Approaches
```bash
# Run all 3 approaches and create comparison table
python evaluate_compare.py
```

## ðŸ“Š Expected Results

| Approach | Accuracy | Speed | Best For |
|----------|----------|-------|----------|
| **spaCy** | 67.9% | 4.2 ms | Production speed âš¡ |
| **Zero-Shot** | 85-90%* | 500-1000 ms | Unknown entities â­ |
| **FinBERT** | 82-88%* | 50-100 ms | Finance context ðŸ’° |

*Estimated based on research (see `NLP_LIBRARY_COMPARISON.md`)

## ðŸ“ Output Files

After running evaluations:
- `results_spacy.json` - spaCy detailed results
- `results_zero_shot.json` - Zero-Shot detailed results
- `results_finbert.json` - FinBERT detailed results
- `comparison_results_*.csv` - Side-by-side comparison table

## ðŸ“ Dataset Details

**81 labeled examples covering:**
- 7 entity types: ORG, PERSON, MONEY, DATE, GPE, LOC, FAC
- 24 entity categories (Bank, School, Hospital, Tech, Retail, etc.)
- 66 well-known entities (DBS, NUS, Twitter, Google, etc.)
- 15 unknown/local entities (Breadtalk, PropertyGuru, Koufu, etc.)

**Entity breakdown:**
- 51 Organizations (banks, schools, companies)
- 6 People (customers, CEOs, employees)
- 7 Money amounts (amounts, currencies)
- 6 Dates (transaction dates, periods)
- 7 Geopolitical entities (countries)
- 3 Locations (streets, places)
- 1 Facility (buildings)

